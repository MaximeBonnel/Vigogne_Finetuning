{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPx_Un8YfNsb"
      },
      "outputs": [],
      "source": [
        "# SETUP destination directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2fz35A2fS_Q"
      },
      "outputs": [],
      "source": [
        "# SETUP environement\n",
        "%%capture\n",
        "!pip install transformers==4.36.0\n",
        "!pip install git+https://github.com/huggingface/accelerate.git -q -U\n",
        "!pip install bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/peft.git -q -U\n",
        "!pip install --no-cache-dir sentencepiece\n",
        "!pip install -q datasets einops wandb trl\n",
        "\n",
        "import sentencepiece, torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65s1_5ExgSWu"
      },
      "outputs": [],
      "source": [
        "# IMPORT DATASET from huggingFace\n",
        "%%capture\n",
        "dataset_name = \"Maxime62/JuniaLLM\"\n",
        "dataset = load_dataset(dataset_name)['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmPj6CYlTLA5"
      },
      "outputs": [],
      "source": [
        "# We will use the Vigogne model\n",
        "# Vigogne is a collection of powerful French large language models (LLMs) that are open-source and designed for instruction-following\n",
        "model_name = \"bofenghuang/vigogne-2-7b-instruct\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"vigogne-2-7b-Junia\"\n",
        "\n",
        "# !!! FineTuning parameters have been optimized with Unsloth !!!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new tokens for our JUNIA LLM\n",
        "new_tokens = [\n",
        "\"ISEN\", \"ISA\", \"HEI\", \"JUNIA\"\n",
        "]"
      ],
      "metadata": {
        "id": "M3CAnlmfTN55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe8ynD9DkavL"
      },
      "outputs": [],
      "source": [
        "# LOADING TOKENIZER\n",
        "max_seq_length = 2048\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    use_fast=False,\n",
        "    truncation=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.bos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVcGpNn0lYlT"
      },
      "outputs": [],
      "source": [
        "# LOADING MODEL\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # dataset load is done in 4-bit\n",
        "    bnb_4bit_quant_type=\"nf4\",# The \"nf4\" value suggests that the model is using \"narrow full\" 4-bit quantization\n",
        "    bnb_4bit_compute_dtype=torch.float16, #computation are done in 16-bit fp\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new tokens to the tokenizer's vocabulary\n",
        "tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "# Resize the token embedding matrix to match the new vocabulary size\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "Nw3s5A8ekcC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-Xb7CLLQ7Dh"
      },
      "outputs": [],
      "source": [
        "# Function is designed to format the elements of a dataset\n",
        "def formatting_prompts_func(examples):\n",
        "    output_text = [] # will hold the formatted text\n",
        "    for i in range(len(examples)):\n",
        "        instruction = examples[\"instruction\"][i]\n",
        "        response = examples[\"output\"][i]\n",
        "\n",
        "        text = f\"\"\"<s>Ci-dessous se trouve une instruction qui décrit une demande d'un étudiant de chez Junia. Rédigez une réponse qui répond de manière précise à la demande.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "{response}</s>\"\"\"\n",
        "\n",
        "        output_text.append(text)\n",
        "    return output_text # return the final output of the formated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy6E3YcQhv72"
      },
      "outputs": [],
      "source": [
        "# LoRA\n",
        "# Setting up hyperparameters for LoraConfig\n",
        "lora_r = 64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2GDG8K0xbbt"
      },
      "outputs": [],
      "source": [
        "# Setting up the training configuration for the model\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_steps = 5,\n",
        "    max_steps = 60,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 1,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 3407,\n",
        "    output_dir = \"./Fine Tuning\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfk9AGElxhuy"
      },
      "outputs": [],
      "source": [
        "# Setting up the trainer for fine-tuning\n",
        "%%capture\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    max_seq_length = max_seq_length,\n",
        "    peft_config=peft_config,\n",
        "    formatting_func=formatting_prompts_func, # Formatting the dataset with the function defined above\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args=training_arguments,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbwAq2NVysNl"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(\"Output\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS37aom_QrvD"
      },
      "outputs": [],
      "source": [
        "# Merge the model and LoRa\n",
        "peftModel = PeftModel.from_pretrained(model, \"Output\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C1WKvepTMxh"
      },
      "outputs": [],
      "source": [
        "# Upload the model on HuggingFace\n",
        "!huggingface-cli login\n",
        "\n",
        "peftModel.push_to_hub(new_model)\n",
        "tokenizer.push_to_hub(new_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}